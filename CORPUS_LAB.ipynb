{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'A concordance is an alphabetical list of the principal words used in a book or body of work, \\nlisting every instance of each word with its immediate context.\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TASK 1 DEMOSTRATING COCA CORPUS\n",
    "''''A concordance is an alphabetical list of the principal words used in a book or body of work, \n",
    "listing every instance of each word with its immediate context.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lemmat'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Morphological analysis is the process of providing grammatical information about the word on the basis of properties of the morpheme it contains. \n",
    "Morphological analyzer and generator are the two essential and basic tools for building any natural language processing application\n",
    "\n",
    "Stemming is the process of reducing a word to its word stem that affixes to suffixes\n",
    "and prefixes or to the roots of words known as a lemma. Stemming is important in natural language understanding (NLU) \n",
    "and natural language processing (NLP).'''\n",
    "#TASK 2 STEMMING:\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "Stemmerporter=PorterStemmer()\n",
    "Stemmerporter.stem('lemmatization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cheer'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''''The Porter stemming algorithm (or 'Porter stemmer') \n",
    "is a process for removing the commoner morphological and inflexional endings from words in English. \n",
    "Its main use is as part of a term normalisation process that is usually done when setting up Information Retrieval systems'''\n",
    "#TASK 2 STEMMING:\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "Stemmerporter=PorterStemmer()\n",
    "Stemmerporter.stem('cheerfulness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happy'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Lancaster: Very aggressive stemming algorithm, sometimes to a fault. \n",
    "With porter and snowball, the stemmed representations are usually fairly intuitive to a reader, \n",
    "not so with Lancaster, as many shorter words will become totally obfuscated'''\n",
    "import nltk\n",
    "from nltk.stem import LancasterStemmer\n",
    "stemmerLan =LancasterStemmer()\n",
    "stemmerLan.stem('happiness')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ing'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''''A stemmer that uses regular expressions to identify morphological affixes. \n",
    "Any substrings that match the regular expressions will be removed.'''\n",
    "import nltk\n",
    "from nltk.stem import RegexpStemmer\n",
    "stemmerregexp=RegexpStemmer('learn')\n",
    "stemmerregexp.stem('learning')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mang'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Snowball. Snowball is a small string processing language designed for creating stemming algorithms \n",
    "for use in Information Retrieval. \n",
    "presents several useful stemmers which have been implemented using it.'''\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "SnowballStemmer.languages\n",
    "frenchstemmer=SnowballStemmer('french')\n",
    "frenchstemmer.stem('manges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Am quick brown fox jump over a lazi dog\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#TASK 3: STEMMING PARAGRAPHS\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "example = \"Am quick brown fox jumps over a lazy dog\"\n",
    "example = [stemmer.stem(token) for token in example.split(\" \")]\n",
    "print (\" \".join(example))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cactus\n",
      "mouse\n",
      "rock\n",
      "good\n",
      "Am\n",
      "be\n"
     ]
    }
   ],
   "source": [
    "'''WordNet is the lexical database i.e.\n",
    "dictionary for the English language, specifically designed for natural language processing. \n",
    "\n",
    "\n",
    "Lemmatization is the process of grouping together the different inflected forms of a word \n",
    "so they can be analysed as a single item. Lemmatization is similar to stemming but it brings context to the words. \n",
    "So it links words with similar meaning to one word.\n",
    "\n",
    "Text preprocessing includes both Stemming as well as Lemmatization. Many times people find these two terms confusing. \n",
    "Some treat these two as same. \n",
    "Actually, lemmatization is preferred over Stemming because lemmatization does morphological analysis of the words.\n",
    "\n",
    "'''\n",
    "# TASK 4: LEMMATIZER\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"mice\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "\n",
    "# a denotes adjective in \"pos\" (part of speech)\n",
    "print(lemmatizer.lemmatize(\"better\", pos = 'a')) # given the part-of-speech, better lemmatizes to good\n",
    "\n",
    "print(lemmatizer.lemmatize(\"Am\"))   # This error is fixed when the PArt of Speech is given\n",
    "\n",
    "print(lemmatizer.lemmatize(\"am\", pos = 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "把 句子 中所 所有 的 可以 成 词 的 词语 都 扫描 描出 描出来 出来\n"
     ]
    }
   ],
   "source": [
    "# TASK 5: CHINESE SEGMENTATION USING JIEBA\n",
    "#Chinese word segmentation module.\n",
    "\n",
    "import jieba\n",
    "seg = jieba.cut(\"把句子中所有的可以成词的词语都扫描出来\", cut_all = True)\n",
    "print(\" \".join(seg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Become', 'an', 'expert', 'in', 'NLP']\n",
      "['The', 'only', 'true', 'wisdom', 'is', 'in', 'knowin', \"'\", 'you', 'know', 'nothing', '.']\n",
      "[('The', 'DT'), ('only', 'JJ'), ('true', 'JJ'), ('wisdom', 'NN'), ('is', 'VBZ'), ('in', 'IN'), ('knowin', 'NN'), (\"'\", \"''\"), ('you', 'PRP'), ('know', 'VBP'), ('nothing', 'NN'), ('.', '.')]\n",
      "['Beware', 'the', 'barrenness', 'of', 'a', 'busy', 'life', '.']\n",
      "[('Beware', 'NNP'), ('the', 'DT'), ('barrenness', 'NN'), ('of', 'IN'), ('a', 'DT'), ('busy', 'JJ'), ('life', 'NN'), ('.', '.')]\n",
      "['I', 'decided', 'that', 'it', 'was', 'not', 'wisdom', 'that', 'enabled', 'poets', 'to', 'write', 'their', 'poetry', ',', 'but', 'a', 'kind', 'of', 'ins', '.']\n",
      "[('I', 'PRP'), ('decided', 'VBD'), ('that', 'IN'), ('it', 'PRP'), ('was', 'VBD'), ('not', 'RB'), ('wisdom', 'JJ'), ('that', 'IN'), ('enabled', 'VBD'), ('poets', 'NNS'), ('to', 'TO'), ('write', 'VB'), ('their', 'PRP$'), ('poetry', 'NN'), (',', ','), ('but', 'CC'), ('a', 'DT'), ('kind', 'NN'), ('of', 'IN'), ('ins', 'NNS'), ('.', '.')]\n",
      "['or', 'inspiration', ',', 'such', 'as', 'you', 'find', 'in', 'seers', 'and', 'prophets', 'who', 'deliver', 'all', 'their', 'sublime', 'messages', 'without', 'knowing', 'in', 'the', 'least', 'what', 'they', 'mean', '.']\n",
      "[('or', 'CC'), ('inspiration', 'NN'), (',', ','), ('such', 'JJ'), ('as', 'IN'), ('you', 'PRP'), ('find', 'VBP'), ('in', 'IN'), ('seers', 'NNS'), ('and', 'CC'), ('prophets', 'NNS'), ('who', 'WP'), ('deliver', 'VBP'), ('all', 'DT'), ('their', 'PRP$'), ('sublime', 'NN'), ('messages', 'NNS'), ('without', 'IN'), ('knowing', 'VBG'), ('in', 'IN'), ('the', 'DT'), ('least', 'JJS'), ('what', 'WP'), ('they', 'PRP'), ('mean', 'VBP'), ('.', '.')]\n",
      "['Be', 'as', 'you', 'wish', 'to', 'seem', '.']\n",
      "[('Be', 'VB'), ('as', 'IN'), ('you', 'PRP'), ('wish', 'VBP'), ('to', 'TO'), ('seem', 'VB'), ('.', '.')]\n",
      "['Wonder', 'is', 'the', 'beginning', 'of', 'wisdom', '.']\n",
      "[('Wonder', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('beginning', 'NN'), ('of', 'IN'), ('wisdom', 'NN'), ('.', '.')]\n",
      "['Be', 'kind', ',', 'for', 'everyone', 'you', 'meet', 'is', 'fighting', 'a', 'hard', 'battle', '.']\n",
      "[('Be', 'NNP'), ('kind', 'NN'), (',', ','), ('for', 'IN'), ('everyone', 'NN'), ('you', 'PRP'), ('meet', 'VBP'), ('is', 'VBZ'), ('fighting', 'VBG'), ('a', 'DT'), ('hard', 'JJ'), ('battle', 'NN'), ('.', '.')]\n",
      "['Our', 'prayers', 'should', 'be', 'for', 'blessings', 'in', 'general', ',', 'for', 'God', 'knows', 'best', 'what', 'is', 'good', 'for', 'us', '.']\n",
      "[('Our', 'PRP$'), ('prayers', 'NNS'), ('should', 'MD'), ('be', 'VB'), ('for', 'IN'), ('blessings', 'NNS'), ('in', 'IN'), ('general', 'JJ'), (',', ','), ('for', 'IN'), ('God', 'NNP'), ('knows', 'VBZ'), ('best', 'JJS'), ('what', 'WP'), ('is', 'VBZ'), ('good', 'JJ'), ('for', 'IN'), ('us', 'PRP'), ('.', '.')]\n",
      "total word [100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Roshan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Roshan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# TASK 6: BASIC TEXT PROCESSING PIPELINE\n",
    "import nltk\n",
    "'''Punkt Sentence Tokenizer. This tokenizer divides a text into a list of sentences, \n",
    "by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences. \n",
    "It must be trained on a large collection of plaintext in the target language before it can be used.'''\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import nltk\n",
    "sent = \"Become an expert in NLP\"\n",
    "words = nltk.word_tokenize(sent)\n",
    "print(words)\n",
    "\n",
    "texts = [\"\"\"The only true wisdom is in knowin' you know nothing.\n",
    "Beware the barrenness of a busy life.\n",
    "I decided that it was not wisdom that enabled poets to write their poetry,\n",
    "but a kind of ins. or inspiration, such as you find in seers and prophets who deliver all their sublime messages without knowing in the least what they mean. Be as you wish to seem. Wonder is the beginning of wisdom. Be kind, for everyone you meet is fighting a hard battle.  Our prayers should be for blessings in general, for God knows best what is good for us.\"\"\"]\n",
    "count=0;\n",
    "for text in texts:\n",
    "    '''Tokenization is the process of tokenizing or splitting a string, text into a list of tokens. \n",
    "    One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph. \n",
    "    How sent_tokenize works ? The sent_tokenize function uses an instance of PunktSentenceTokenizer from the nltk'''\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        '''NLTK provides a function called word_tokenize() for splitting strings into tokens (nominally words). \n",
    "        It splits tokens based on white space and punctuation. \n",
    "        For example, commas and periods are taken as separate tokens'''\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        print(words)\n",
    "        tagged = nltk.pos_tag(words)\n",
    "        print(tagged)\n",
    "num_words = [len(sentence.split()) for sentence in texts]\n",
    "print('total word',num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
